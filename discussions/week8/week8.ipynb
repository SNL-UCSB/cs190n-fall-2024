{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e628aa0",
   "metadata": {},
   "source": [
    "# Preprocess, train and use Trustee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd12220",
   "metadata": {},
   "source": [
    "## Imports for pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0eaf464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a64d8",
   "metadata": {},
   "source": [
    "## Constants (Directly imported from the puffer paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7561418",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DURATION = 180180\n",
    "PKT_BYTES = 1500\n",
    "MILLION = 1000000\n",
    "PAST_CHUNKS = 8\n",
    "FUTURE_CHUNKS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622283df",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "    1. 2 CSVs video sent and video acked\n",
    "    2. Parse them to relevant data types\n",
    "    3. calculate the transmission time from sent and acked\n",
    "    4. Consider the past 8 chunks and pad the values if necessary\n",
    "    5. Create the examples for the future chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4cb378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_data(video_sent_path, video_acked_path, time_start=None, time_end=None):\n",
    "    \"\"\"\n",
    "    Load data from files and calculate chunk transmission times.\n",
    "    \"\"\"\n",
    "    video_sent_df = pd.read_csv(video_sent_path)\n",
    "    video_acked_df = pd.read_csv(video_acked_path)\n",
    "\n",
    "    # Rename \"time (ns GMT)\" to \"time\" for convenience\n",
    "    video_sent_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "    video_acked_df.rename(columns={'time (ns GMT)': 'time'}, inplace=True)\n",
    "\n",
    "    # Convert nanosecond timestamps to datetime\n",
    "    video_sent_df['time'] = pd.to_datetime(video_sent_df['time'], unit='ns')\n",
    "    video_acked_df['time'] = pd.to_datetime(video_acked_df['time'], unit='ns')\n",
    "\n",
    "    # Filter by time range\n",
    "    if time_start:\n",
    "        time_start = pd.to_datetime(time_start)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] >= time_start]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] >= time_start]\n",
    "    if time_end:\n",
    "        time_end = pd.to_datetime(time_end)\n",
    "        video_sent_df = video_sent_df[video_sent_df['time'] <= time_end]\n",
    "        video_acked_df = video_acked_df[video_acked_df['time'] <= time_end]\n",
    "\n",
    "    # Process the data\n",
    "    return calculate_trans_times(video_sent_df, video_acked_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bcdb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trans_times(video_sent_df, video_acked_df):\n",
    "    \"\"\"\n",
    "    Calculate transmission times from video_sent and video_acked datasets using session_id.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    last_video_ts = {}\n",
    "\n",
    "    for _, row in video_sent_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id to track sessions\n",
    "        if session not in d:\n",
    "            d[session] = {}\n",
    "            last_video_ts[session] = None\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if last_video_ts[session] is not None:\n",
    "            if video_ts != last_video_ts[session] + VIDEO_DURATION:\n",
    "                continue\n",
    "\n",
    "        last_video_ts[session] = video_ts\n",
    "        d[session][video_ts] = {\n",
    "            'sent_ts': pd.Timestamp(row['time']),\n",
    "            'size': float(row['size']) / PKT_BYTES,\n",
    "            'delivery_rate': float(row['delivery_rate']) / PKT_BYTES,\n",
    "            'cwnd': float(row['cwnd']),\n",
    "            'in_flight': float(row['in_flight']),\n",
    "            'min_rtt': float(row['min_rtt']) / MILLION,\n",
    "            'rtt': float(row['rtt']) / MILLION,\n",
    "        }\n",
    "\n",
    "    for _, row in video_acked_df.iterrows():\n",
    "        session = row['session_id']  # Use only session_id\n",
    "        if session not in d:\n",
    "            continue\n",
    "\n",
    "        video_ts = int(row['video_ts'])\n",
    "        if video_ts not in d[session]:\n",
    "            continue\n",
    "\n",
    "        dsv = d[session][video_ts]\n",
    "        sent_ts = dsv['sent_ts']\n",
    "        acked_ts = pd.Timestamp(row['time'])\n",
    "        dsv['acked_ts'] = acked_ts\n",
    "        dsv['trans_time'] = (acked_ts - sent_ts).total_seconds()\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3ee73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_past_chunks(ds, next_ts, row):\n",
    "    i = 1\n",
    "    past_chunks = []\n",
    "    while i <= PAST_CHUNKS:\n",
    "        ts = next_ts - i * VIDEO_DURATION\n",
    "        if ts in ds and 'trans_time' in ds[ts]:\n",
    "            past_chunks = [ds[ts]['delivery_rate'],\n",
    "                           ds[ts]['cwnd'], ds[ts]['in_flight'],\n",
    "                           ds[ts]['min_rtt'], ds[ts]['rtt'],\n",
    "                           ds[ts]['size'], ds[ts]['trans_time']] + past_chunks\n",
    "        else:\n",
    "            nts = ts + VIDEO_DURATION  # padding with the nearest ts\n",
    "            padding = [ds[nts]['delivery_rate'],\n",
    "                       ds[nts]['cwnd'], ds[nts]['in_flight'],\n",
    "                       ds[nts]['min_rtt'], ds[nts]['rtt']]\n",
    "            if nts == next_ts:\n",
    "                padding += [0, 0]  # next_ts is the first chunk to send\n",
    "            else:\n",
    "                padding += [ds[nts]['size'], ds[nts]['trans_time']]\n",
    "            break\n",
    "        i += 1\n",
    "    if i != PAST_CHUNKS + 1:  # break in the middle; padding must exist\n",
    "        while i <= PAST_CHUNKS:\n",
    "            past_chunks = padding + past_chunks\n",
    "            i += 1\n",
    "    row += past_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b6741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_output(d):\n",
    "    ret = [{'in': [], 'out': []} for _ in range(5)]  # FUTURE_CHUNKS = 5\n",
    "\n",
    "    for session in d:\n",
    "        ds = d[session]\n",
    "\n",
    "        for next_ts in ds:\n",
    "            if 'trans_time' not in ds[next_ts]:\n",
    "                continue\n",
    "\n",
    "            row = []\n",
    "\n",
    "            # Append past chunks\n",
    "            append_past_chunks(ds, next_ts, row)\n",
    "\n",
    "            # Append the TCP info of the next chunk\n",
    "            row += [ds[next_ts]['delivery_rate'],\n",
    "                    ds[next_ts]['cwnd'], ds[next_ts]['in_flight'],\n",
    "                    ds[next_ts]['min_rtt'], ds[next_ts]['rtt']]\n",
    "\n",
    "            # Generate FUTURE_CHUNKS rows\n",
    "            for i in range(5):  # FUTURE_CHUNKS = 5\n",
    "                row_i = row.copy()\n",
    "\n",
    "                ts = next_ts + i * VIDEO_DURATION\n",
    "                if ts in ds and 'trans_time' in ds[ts]:\n",
    "                    row_i += [ds[ts]['size']]\n",
    "\n",
    "                    ret[i]['in'].append(row_i)\n",
    "                    ret[i]['out'].append(ds[ts]['trans_time'])\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf39386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(output_file, processed_data):\n",
    "    \"\"\"\n",
    "    Save processed data to a file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22170331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    DEFAULT_VIDEO_SENT_PATH = '/mnt/md0/jaber/puffer_train/video_sent_2022-10-01T11_2022-10-02T11.csv'\n",
    "    DEFAULT_VIDEO_ACKED_PATH = '/mnt/md0/jaber/puffer_train/video_acked_2022-10-01T11_2022-10-02T11.csv'\n",
    "    DEFAULT_OUTPUT_FILE = '/mnt/md0/<user_id>/old_output.pkl'\n",
    "    \n",
    "    #Latest datasets can be found at https://puffer.stanford.edu/results/\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"Process video streaming datasets.\")\n",
    "    parser.add_argument('--video_sent_path', type=str, help='Path to the video_sent dataset CSV file.')\n",
    "    parser.add_argument('--video_acked_path', type=str, help='Path to the video_acked dataset CSV file.')\n",
    "    parser.add_argument('--output_file', type=str, help='Path to save the processed data.')\n",
    "    parser.add_argument('--time_start', type=str, default=None, help='Start time for filtering data (RFC3339 format).')\n",
    "    parser.add_argument('--time_end', type=str, default=None, help='End time for filtering data (RFC3339 format).')\n",
    "    #args = parser.parse_args()\n",
    "    #processed_data = prepare_input_output(prepare_raw_data(args.video_sent_path, args.video_acked_path,\n",
    "    #    time_start=args.time_start, time_end=args.time_end))\n",
    "    # save_processed_data(args.output_file, processed_data)\n",
    "    processed_data = prepare_input_output(prepare_raw_data(DEFAULT_VIDEO_SENT_PATH, DEFAULT_VIDEO_ACKED_PATH,\n",
    "        time_start=None, time_end=None))\n",
    "    save_processed_data(DEFAULT_OUTPUT_FILE, processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe3a6d",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "1. Define the model.\n",
    "2. Normalize the data\n",
    "3. Discretize the labels\n",
    "4. Plot the classification report\n",
    "5. Use trustee to identify the important features\n",
    "6. Plot the Trustee tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model side imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os import path\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from trustee import ClassificationTrustee\n",
    "import graphviz\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 500\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "inference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cb03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    #Model constants\n",
    "    PAST_CHUNKS = 8\n",
    "    FUTURE_CHUNKS = 5\n",
    "    DIM_IN = 62\n",
    "    COLUMNS = [j + str(i) for i in range(PAST_CHUNKS + 1) for j in ['delivery_rate', 'cwnd', 'in_flight', 'min_rtt', 'rtt', 'size', 'trans_time']][:DIM_IN]\n",
    "    DIM_OUT = 21  # BIN_MAX + 1\n",
    "    DIM_H1 = 64\n",
    "    DIM_H2 = 64\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    BIN_SIZE = 0.5  # seconds\n",
    "    BIN_MAX = 20\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(Model.DIM_IN, Model.DIM_H1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Model.DIM_H1, Model.DIM_H2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(Model.DIM_H2, Model.DIM_OUT),\n",
    "        ).double().to(device=DEVICE)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().to(device=DEVICE)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),\n",
    "                                          lr=Model.LEARNING_RATE,\n",
    "                                          weight_decay=Model.WEIGHT_DECAY)\n",
    "        self.obs_size = None\n",
    "        self.obs_mean = None\n",
    "        self.obs_std = None\n",
    "\n",
    "    def update_obs_stats(self, raw_in):\n",
    "        if self.obs_size is None:\n",
    "            self.obs_size = len(raw_in)\n",
    "            self.obs_mean = np.mean(raw_in, axis=0)\n",
    "            self.obs_std = np.std(raw_in, axis=0)\n",
    "            return\n",
    "        old_size = self.obs_size\n",
    "        new_size = len(raw_in)\n",
    "        self.obs_size = old_size + new_size\n",
    "        old_mean = self.obs_mean\n",
    "        new_mean = np.mean(raw_in, axis=0)\n",
    "        self.obs_mean = (old_mean * old_size + new_mean * new_size) / self.obs_size\n",
    "        old_std = self.obs_std\n",
    "        old_sum_square = old_size * (np.square(old_std) + np.square(old_mean))\n",
    "        new_sum_square = np.sum(np.square(raw_in), axis=0)\n",
    "        mean_square = (old_sum_square + new_sum_square) / self.obs_size\n",
    "        self.obs_std = np.sqrt(mean_square - np.square(self.obs_mean))\n",
    "\n",
    "    def normalize_input(self, raw_in, update_obs=False):\n",
    "        z = np.array(raw_in)\n",
    "        if update_obs:\n",
    "            self.update_obs_stats(z)\n",
    "        assert self.obs_size is not None\n",
    "        for col in range(len(self.obs_mean)):\n",
    "            z[:, col] -= self.obs_mean[col]\n",
    "            if self.obs_std[col] != 0:\n",
    "                z[:, col] /= self.obs_std[col]\n",
    "        return z\n",
    "\n",
    "    def discretize_output(self, raw_out):\n",
    "        z = np.array(raw_out)\n",
    "        z = np.floor((z + 0.5 * Model.BIN_SIZE) / Model.BIN_SIZE).astype(int)\n",
    "        return np.clip(z, 0, Model.BIN_MAX)\n",
    "\n",
    "    def train(self, train_input, train_output, test_input, test_output):\n",
    "        train_input = torch.from_numpy(self.normalize_input(train_input, update_obs=inference)).to(DEVICE)\n",
    "        train_output = torch.from_numpy(self.discretize_output(train_output)).to(DEVICE)\n",
    "        test_input = torch.from_numpy(self.normalize_input(test_input, update_obs=False)).to(DEVICE)\n",
    "        test_output = torch.from_numpy(self.discretize_output(test_output)).to(DEVICE)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            self.model.train()\n",
    "            perm = np.random.permutation(len(train_input))\n",
    "            train_input = train_input[perm]\n",
    "            train_output = train_output[perm]\n",
    "\n",
    "            num_batches = int(np.ceil(len(train_input) / BATCH_SIZE))\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * BATCH_SIZE\n",
    "                end_idx = min((i + 1) * BATCH_SIZE, len(train_input))\n",
    "\n",
    "                batch_input = train_input[start_idx:end_idx]\n",
    "                batch_output = train_output[start_idx:end_idx]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.model(batch_input)\n",
    "                loss = self.loss_fn(predictions, batch_output)\n",
    "\n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {epoch_loss / num_batches}\")\n",
    "\n",
    "            # Evaluate after each epoch\n",
    "            self.evaluate(test_input, test_output)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        checkpoint = torch.load(model_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        self.obs_size = checkpoint['obs_size']\n",
    "        self.obs_mean = checkpoint['obs_mean']\n",
    "        self.obs_std = checkpoint['obs_std']\n",
    "\n",
    "    def save(self, model_path):\n",
    "        assert (self.obs_size is not None)\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'obs_size': self.obs_size,\n",
    "            'obs_mean': self.obs_mean,\n",
    "            'obs_std': self.obs_std,\n",
    "        }, model_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.detach().cpu().numpy()\n",
    "            return ret\n",
    "    def predict_discrete(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.detach().cpu().numpy()\n",
    "            return y_scores, ret\n",
    "\n",
    "    def predict_cont(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = x.to_numpy()\n",
    "            x = self.normalize_input(x, update_obs=inference)\n",
    "            x = torch.from_numpy(x).to(DEVICE)\n",
    "            y_scores = self.model(x)\n",
    "            y_predicted = torch.max(y_scores, 1)[1].to(device=DEVICE)\n",
    "            ret = y_predicted.double().numpy()\n",
    "            for i in range(len(ret)):\n",
    "                bin_id = ret[i]\n",
    "                if bin_id == 0:  # the first bin is defined differently\n",
    "                    ret[i] = 0.25 * Model.BIN_SIZE\n",
    "                else:\n",
    "                    ret[i] = bin_id * Model.BIN_SIZE\n",
    "            return ret\n",
    "\n",
    "    def evaluate_with_trustee(self, test_input, test_output):\n",
    "        self.model.eval()\n",
    "        pd_input = pd.DataFrame(test_input, columns=self.COLUMNS)\n",
    "        test_output_discretized = self.discretize_output(test_output)\n",
    "        with torch.no_grad():\n",
    "            predictions_prob, class_preds = self.predict_discrete(pd_input)\n",
    "            # Cross-Entropy Loss\n",
    "            cross_entropy_loss = self.loss_fn(predictions_prob, torch.from_numpy(test_output_discretized)).item()\n",
    "            # Print metrics\n",
    "            print(f\"Test Cross-Entropy Loss: {cross_entropy_loss}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(test_output_discretized, class_preds, zero_division=0))\n",
    "            trustee = ClassificationTrustee(expert=model)\n",
    "            trustee.fit(pd_input, test_output_discretized, num_iter=10, num_stability_iter=2, samples_size=0.3, verbose=True)\n",
    "            dt, pruned_dt, agreement, reward = trustee.explain()\n",
    "            dt_y_pred = dt.predict(pd_input)\n",
    "            print(\"Model explanation global fidelity report:\")\n",
    "            print(classification_report(class_preds, dt_y_pred))\n",
    "            dot_data = tree.export_graphviz(pruned_dt, class_names=[str(i)for i in range(21)], feature_names=model.COLUMNS,filled=True,rounded=True,special_characters=True)\n",
    "            graph = graphviz.Source(dot_data)\n",
    "            fil = graph.render(\"~/trustee_tree_puffer_pruned\", format=\"png\")\n",
    "\n",
    "    def evaluate(self, test_input, test_output):\n",
    "        self.model.eval()\n",
    "        pd_input = pd.DataFrame(test_input, columns=self.COLUMNS)\n",
    "        with torch.no_grad():\n",
    "            predictions = self.predict_cont(pd_input)\n",
    "            # Print metrics\n",
    "\n",
    "            # Mean Squared Error\n",
    "            mse_loss = mean_squared_error(test_output, predictions)\n",
    "            print(f\"Test Mean Squared Error: {mse_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/md0/satya/old_output.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "model = Model()\n",
    "\n",
    "for i, chunk_data in enumerate(processed_data):\n",
    "    input_data = np.array(chunk_data['in'])\n",
    "    output_data = np.array(chunk_data['out'])\n",
    "    if not inference:\n",
    "        print(f\"Training model for future chunk {i + 1}\")\n",
    "        # Train-test split (70% train, 30% test)\n",
    "        train_input, test_input, train_output, test_output = train_test_split(\n",
    "            input_data, output_data, test_size=0.3, random_state=42\n",
    "        )\n",
    "        model.train(train_input, train_output, test_input, test_output)\n",
    "    else:\n",
    "        model.load(\"/mnt/md0/satya/puffer/src/model/py-0-checkpoint-200.pt\")\n",
    "        # the model is available at https://storage.googleapis.com/puffer-models/puffer-ttp/bbr-20221001-1.tar.gz\n",
    "        model.evaluate(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99179fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_with_trustee(input_data, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5c7b1",
   "metadata": {},
   "source": [
    "![title](trustee_tree_puffer_pruned.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae19dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
